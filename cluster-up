#!/bin/bash
source helpers

mkdir -p ~/.kube

first_node="hunter"
master_nodes=("wrecker" "crosshair")
agent_nodes=("elendil" "aragorn" "isildur" "durin" "faramir" "boromir" "cmdrcody" "tech")

# install the first master / control-plane node

./add-control-plane-node $first_node $first_node

# grab a local copy of the kube config from the first master node
ssh $first_node 'mkdir -p .kube; sudo cp /etc/rancher/k3s/k3s.yaml /home/gerry/.kube/config; sudo chown gerry:gerry .kube/config; sudo chmod 600 .kube/config;'
scp $first_node:.kube/config ~/.kube/config
# modify the kube config so it can be used on machines other than the master node
# this is pinned to the first master node until the remaining control plane nodes are live
yq -i '.clusters[0].cluster.server = "https://'$first_node':6443"|.clusters[0].name = "middle-earth"|.contexts[0].name = "middle-earth"|.contexts[0].context.cluster = "middle-earth"|.current-context = "middle-earth"' ~/.kube/config

# install cilium - requires the cilium CLI installed locally

[ -x "$(command -v cilium)" ] || {
    echoci -c red -d "cilium CLI not found - please install it"
    exit 1
}

cilium install --version 1.16.5 --set=ipam.operator.clusterPoolIPv4PodCIDRList="10.42.0.0/16"

# check that coredns is running before proceeding to add the remaining control plane nodes
echoci -c cyan -d "waiting for coredns to start"

max_retries=20
retry_count=0
retry_interval=10
while true; do
    [ "$(kubectl -n kube-system get po -l k8s-app=kube-dns -o json | jq -r '.items[]|.status.phase')" = "Running" ] && break
    retry_count=$(($retry_count + 1))
    [ $retry_count -gt $max_retries ] && {
        echoci -c red -d "coredns is not running - stopping build";
        exit 1;
    }
    echoci -c yellow -d "coredns not running yet - waiting $retry_interval seconds"
    sleep $retry_interval
done

echoci -c green -d "first node $first_node is up!"

# Install the remaining control-plane nodes one at a time
for node in "${master_nodes[@]}"; do
    ./add-control-plane-node $first_node $node || {
        echoci -c red -d "failed to install control plane node $node"
        exit 1
    }

    max_retries=30
    retry_count=0
    retry_interval=15
    while true; do
        node_ready=$(kubectl get node $node -o json | jq -r '.status.conditions[]|select(.type|match("Ready")).status')
        [ "$node_ready" = "True" ] && {
            echoci -c green -d "node $node is ready"
            break
        }

        retry_count=$(($retry_count + 1))
        [ $retry_count -gt $max_retries ] && {
            echoci -c red -d "node $node did not start in time - stopping build";
            exit 1;
        }

        echoci -c yellow -d "node $node not ready yet - waiting $retry_interval seconds"
        sleep $retry_interval
    done
done

# finish by installing the agent nodes
for node in "${agent_nodes[@]}"; do
    ./add-agent-node $first_node $node
done


# create the cert manager namespace early so we can pre-load the cloudflare secret (the other manifests will create their own namespaces)
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: cert-manager
EOF

./create-cloudflare-secret | kubectl apply -f -
echoci -c cyan -d "deploying manifests for remaining components"

# move the manifests out to the first master node to load
ssh $first_node 'mkdir -p manifests'
scp manifests/* $first_node:manifests/
ssh $first_node sudo cp manifests/* /var/lib/rancher/k3s/server/manifests

# wait for the ingress-nginx pods to start - this affirms that all workers are ready so we can continue with longhorn prep
echoci -c cyan -d "Waiting for ingress-nginx to start"

max_retries=30
retry_count=0
retry_interval=15
while true; do
    ingress_ready_count=$(kubectl get po -n ingress-nginx -o json | jq -r '.items[]|.status.phase' | grep -c Running)
    [ "$ingress_ready_count" -eq ${#agent_nodes[@]} ] && {
        echoci -c green -d "all ingress pods are ready"
        break
    }

    retry_count=$(($retry_count + 1))
    [ $retry_count -gt $max_retries ] && {
        echoci -c red -d "full set of ingress pods did not start in time - stopping build";
        exit 1;
    }

    echoci -c yellow -d "$ingress_ready_count pods ready, expected ${#agent_nodes[@]} - waiting $retry_interval seconds"
    sleep $retry_interval
done

# rebuild the haproxy config to capture any IP address change on the new nodes
./build-haproxy-config

# configure nodes with longhorn prequisites
longhornctl --kube-config ~/.kube/config install preflight
longhornctl --kube-config ~/.kube/config check preflight

# install longhorn
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.7.2/deploy/longhorn.yaml

# install the cloudnative-pg operator
kubectl apply --server-side -f https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.24/releases/cnpg-1.24.2.yaml

# update the kube config to use the domain name
sed -i "s/$first_node/middle-earth.g2dev.net/" ~/.kube/config
# copy back to the first master node so other clients can grab latest with mods
scp ~/.kube/config $first_node:.kube/

